{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import csv\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "from statsmodels.regression.linear_model import RegressionResults\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train(df): \n",
    "    df['future_tweets'] = df['tweets']\n",
    "    df.future_tweets = df.future_tweets.shift(-1)\n",
    "    df.future_tweets[-1] = df.future_tweets[-2]\n",
    "    target = df['future_tweets'].values.tolist()\n",
    "    target = np.asarray(target)\n",
    "    df = df.drop(['future_tweets'], axis=1)\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(df.values.tolist(), target)\n",
    "    return regr\n",
    "\n",
    "def mlp_train(df):\n",
    "    df['future_tweets'] = df['tweets']\n",
    "    df.future_tweets = df.future_tweets.shift(-1)\n",
    "    df.future_tweets[-1] = df.future_tweets[-2]\n",
    "    target = df['future_tweets'].values.tolist()\n",
    "    target = np.asarray(target)\n",
    "    df = df.drop(['future_tweets'], axis=1)\n",
    "    mlp = MLPRegressor()\n",
    "    mlp.fit(df.values.tolist(), target)\n",
    "    return mlp\n",
    "\n",
    "def rf_train(df):\n",
    "    df['future_tweets'] = df['tweets']\n",
    "    df.future_tweets = df.future_tweets.shift(-1)\n",
    "    df.future_tweets[-1] = df.future_tweets[-2]\n",
    "    target = df['future_tweets'].values.tolist()\n",
    "    target = np.asarray(target)\n",
    "    df = df.drop(['future_tweets'], axis=1)\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(df.values.tolist(), target)\n",
    "    return rf\n",
    "\n",
    "def cv_score(df, model):\n",
    "    df['future_tweets'] = df['tweets']\n",
    "    df.future_tweets = df.future_tweets.shift(-1)\n",
    "    df.future_tweets[-1] = df.future_tweets[-2]\n",
    "    target = df['future_tweets'].values.tolist()\n",
    "    target = np.asarray(target)\n",
    "    df = df.drop(['future_tweets'], axis=1)\n",
    "    scores = cross_val_score(model, df.values.tolist(), target, cv=2)\n",
    "    return np.sum(scores)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['ECE219_tweet_test/sample0_period1.txt',\n",
    "            'ECE219_tweet_test/sample0_period2.txt',\n",
    "            'ECE219_tweet_test/sample0_period3.txt',\n",
    "            'ECE219_tweet_test/sample1_period1.txt',\n",
    "            'ECE219_tweet_test/sample1_period2.txt',\n",
    "            'ECE219_tweet_test/sample1_period3.txt',\n",
    "            'ECE219_tweet_test/sample2_period1.txt',\n",
    "            'ECE219_tweet_test/sample2_period2.txt',\n",
    "            'ECE219_tweet_test/sample2_period3.txt'\n",
    "            ]\n",
    "\n",
    "output_filenames = ['sample0_pre_active.txt','sample0_active.txt','sample0_post_active.txt',\n",
    "                   'sample1_pre_active.txt','sample1_active.txt','sample1_post_active.txt',\n",
    "                   'sample2_pre_active.txt','sample2_active.txt','sample2_post_active.txt']\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    pst_tz = pytz.timezone('US/Pacific')\n",
    "    \n",
    "    with open(filenames[i], 'r') as reader:\n",
    "        headers = ['citation_date_raw','author_nick_names','tweets','retweets','followers','followers_max']\n",
    "        with open(output_filenames[i], 'w') as writer:\n",
    "            csv_writer = csv.writer(writer, lineterminator='\\n')\n",
    "            csv_writer.writerow(headers)\n",
    "            for line in reader:\n",
    "                data = json.loads(line)\n",
    "                response =  [data['citation_date'],data['author']['nick'],1, data['metrics']['citations']['total'], data['author']['followers'], data['author']['followers'],]\n",
    "                csv_writer.writerow(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preactive0 = pd.read_csv('sample0_pre_active.txt',delimiter=',')\n",
    "active0 = pd.read_csv('sample0_active.txt',delimiter=',')\n",
    "postactive0 = pd.read_csv('sample0_post_active.txt',delimiter=',')\n",
    "\n",
    "preactive1 = pd.read_csv('sample1_pre_active.txt',delimiter=',')\n",
    "active1 = pd.read_csv('sample1_active.txt',delimiter=',')\n",
    "postactive1 = pd.read_csv('sample1_post_active.txt',delimiter=',')\n",
    "\n",
    "preactive2 = pd.read_csv('sample2_pre_active.txt',delimiter=',')\n",
    "active2 = pd.read_csv('sample2_active.txt',delimiter=',')\n",
    "postactive2 = pd.read_csv('sample2_post_active.txt',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_pst(df):\n",
    "    utc_tz = pytz.UTC\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    df['citation_date'] = pd.to_datetime(df['citation_date_raw'], unit='s')\n",
    "    df['citation_date_pst'] = pd.to_datetime(df['citation_date_raw'], unit='s').apply(lambda x: x.tz_localize(utc_tz).astimezone(pst_tz))\n",
    "    df['date'] = df['citation_date_pst'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "    df['hour'] = df['citation_date_pst'].apply(lambda x: x.hour)\n",
    "    df['minute'] = df['citation_date_pst'].apply(lambda x: x.minute)\n",
    "    \n",
    "parse_date_pst(preactive0)\n",
    "parse_date_pst(active0)\n",
    "parse_date_pst(postactive0)\n",
    "\n",
    "parse_date_pst(preactive1)\n",
    "parse_date_pst(active1)\n",
    "parse_date_pst(postactive1)\n",
    "\n",
    "parse_date_pst(preactive2)\n",
    "parse_date_pst(active2)\n",
    "parse_date_pst(postactive2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preactive0_agg = preactive0.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "active0_agg = active0.groupby(pd.Grouper(key='citation_date_pst',freq='5Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "postactive0_agg = postactive0.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "preactive1_agg = preactive1.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "active1_agg = active1.groupby(pd.Grouper(key='citation_date_pst',freq='5Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "postactive1_agg = postactive1.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "preactive2_agg = preactive2.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "active2_agg = active2.groupby(pd.Grouper(key='citation_date_pst',freq='5Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})\n",
    "postactive2_agg = postactive2.groupby(pd.Grouper(key='citation_date_pst',freq='60Min')).agg({'hour':'max','tweets':'sum','retweets':'sum','followers':'sum','followers_max':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>tweets</th>\n",
       "      <th>retweets</th>\n",
       "      <th>followers</th>\n",
       "      <th>followers_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citation_date_pst</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-04 00:00:00-08:00</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>74</td>\n",
       "      <td>3564378.0</td>\n",
       "      <td>3329958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-04 01:00:00-08:00</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>103</td>\n",
       "      <td>146905.0</td>\n",
       "      <td>59219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-04 02:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>99613.0</td>\n",
       "      <td>16753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-04 03:00:00-08:00</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>61</td>\n",
       "      <td>119754.0</td>\n",
       "      <td>27669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-04 04:00:00-08:00</th>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>141</td>\n",
       "      <td>505238.0</td>\n",
       "      <td>257549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-04 05:00:00-08:00</th>\n",
       "      <td>5</td>\n",
       "      <td>87</td>\n",
       "      <td>131</td>\n",
       "      <td>164136.0</td>\n",
       "      <td>27199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 20:00:00-08:00</th>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "      <td>1285</td>\n",
       "      <td>10709633.0</td>\n",
       "      <td>9677129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 21:00:00-08:00</th>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>98</td>\n",
       "      <td>1846822.0</td>\n",
       "      <td>1458675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 22:00:00-08:00</th>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>48886.0</td>\n",
       "      <td>5022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 23:00:00-08:00</th>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>73474.0</td>\n",
       "      <td>32613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-06 00:00:00-08:00</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "      <td>155066.0</td>\n",
       "      <td>36269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-06 01:00:00-08:00</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>56</td>\n",
       "      <td>102647.0</td>\n",
       "      <td>13628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 17:00:00-08:00</th>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "      <td>99</td>\n",
       "      <td>159589.0</td>\n",
       "      <td>68788.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 18:00:00-08:00</th>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>166</td>\n",
       "      <td>930938.0</td>\n",
       "      <td>494835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 19:00:00-08:00</th>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>174742.0</td>\n",
       "      <td>114845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 20:00:00-08:00</th>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "      <td>1285</td>\n",
       "      <td>10709633.0</td>\n",
       "      <td>9677129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 21:00:00-08:00</th>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>98</td>\n",
       "      <td>1846822.0</td>\n",
       "      <td>1458675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-05 22:00:00-08:00</th>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>48886.0</td>\n",
       "      <td>5022.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hour  tweets  retweets   followers  followers_max\n",
       "citation_date_pst                                                           \n",
       "2015-02-04 00:00:00-08:00     0      59        74   3564378.0      3329958.0\n",
       "2015-02-04 01:00:00-08:00     1      48       103    146905.0        59219.0\n",
       "2015-02-04 02:00:00-08:00     2      94        99     99613.0        16753.0\n",
       "2015-02-04 03:00:00-08:00     3      45        61    119754.0        27669.0\n",
       "2015-02-04 04:00:00-08:00     4      77       141    505238.0       257549.0\n",
       "2015-02-04 05:00:00-08:00     5      87       131    164136.0        27199.0\n",
       "2015-02-05 20:00:00-08:00    20      58      1285  10709633.0      9677129.0\n",
       "2015-02-05 21:00:00-08:00    21      87        98   1846822.0      1458675.0\n",
       "2015-02-05 22:00:00-08:00    22      43        43     48886.0         5022.0\n",
       "2015-02-05 23:00:00-08:00    23      27        30     73474.0        32613.0\n",
       "2015-02-06 00:00:00-08:00     0      44        62    155066.0        36269.0\n",
       "2015-02-06 01:00:00-08:00     1      46        56    102647.0        13628.0\n",
       "2015-02-05 17:00:00-08:00    17      81        99    159589.0        68788.0\n",
       "2015-02-05 18:00:00-08:00    18      90       166    930938.0       494835.0\n",
       "2015-02-05 19:00:00-08:00    19      40        50    174742.0       114845.0\n",
       "2015-02-05 20:00:00-08:00    20      58      1285  10709633.0      9677129.0\n",
       "2015-02-05 21:00:00-08:00    21      87        98   1846822.0      1458675.0\n",
       "2015-02-05 22:00:00-08:00    22      43        43     48886.0         5022.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preactive0_comb_agg = preactive0_agg\n",
    "preactive1_comb_agg = pd.concat([preactive0_agg, preactive1_agg])\n",
    "preactive1_comb_agg.dropna()\n",
    "preactive2_comb_agg = pd.concat([preactive0_agg, preactive1_agg, preactive2_agg])\n",
    "preactive2_comb_agg.dropna()\n",
    "\n",
    "active0_comb_agg = active0_agg\n",
    "active1_comb_agg = pd.concat([active0_agg, active1_agg])\n",
    "active1_comb_agg.dropna()\n",
    "active2_comb_agg = pd.concat([active0_agg, active1_agg, active2_agg])\n",
    "active2_comb_agg.dropna()\n",
    "\n",
    "postactive0_comb_agg = postactive0_agg\n",
    "postactive1_comb_agg = pd.concat([postactive0_agg, postactive1_agg])\n",
    "postactive1_comb_agg.dropna()\n",
    "postactive2_comb_agg = pd.concat([postactive0_agg, postactive1_agg, postactive2_agg])\n",
    "postactive2_comb_agg.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3964.4305665692905\n",
      "-789.5342950093461\n",
      "-27.204891143622277\n",
      "-77.56607905811335\n",
      "-335.9576275574258\n",
      "-119.76309265937948\n",
      "-55260.466320882675\n",
      "-34.112228923066155\n",
      "-1.3901796354207405\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preactive0_lr = lr_train(preactive0_comb_agg)\n",
    "print(cv_score(preactive0_comb_agg, preactive0_lr))\n",
    "\n",
    "preactive1_lr = lr_train(preactive1_comb_agg)\n",
    "print(cv_score(preactive1_comb_agg, preactive1_lr))\n",
    "\n",
    "preactive2_lr = lr_train(preactive2_comb_agg)\n",
    "print(cv_score(preactive2_comb_agg, preactive2_lr))\n",
    "\n",
    "active0_lr = lr_train(active0_comb_agg)\n",
    "print(cv_score(active0_comb_agg, active0_lr))\n",
    "\n",
    "active1_lr = lr_train(active1_comb_agg)\n",
    "print(cv_score(active1_comb_agg, active1_lr))\n",
    "\n",
    "active2_lr = lr_train(active2_comb_agg)\n",
    "print(cv_score(active2_comb_agg, active2_lr))\n",
    "\n",
    "postactive0_lr = lr_train(postactive0_comb_agg)\n",
    "print(cv_score(postactive0_comb_agg, postactive0_lr))\n",
    "\n",
    "postactive1_lr = lr_train(postactive1_comb_agg)\n",
    "print(cv_score(postactive1_comb_agg, postactive1_lr))\n",
    "\n",
    "postactive2_lr = lr_train(postactive2_comb_agg)\n",
    "print(cv_score(postactive2_comb_agg, postactive2_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11098357515.685926\n",
      "-728848.9074233344\n",
      "-6394958.677974194\n",
      "-294826.2677092392\n",
      "-7270613.401476458\n",
      "-435.8861432219985\n",
      "-4962846.376971026\n",
      "-72584825.58007556\n",
      "-13624441.00803786\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preactive0_mlp = mlp_train(preactive0_comb_agg)\n",
    "print(cv_score(preactive0_comb_agg, preactive0_mlp))\n",
    "\n",
    "preactive1_mlp = mlp_train(preactive1_comb_agg)\n",
    "print(cv_score(preactive1_comb_agg, preactive1_mlp))\n",
    "\n",
    "preactive2_mlp = mlp_train(preactive2_comb_agg)\n",
    "print(cv_score(preactive2_comb_agg, preactive2_mlp))\n",
    "\n",
    "active0_mlp = mlp_train(active0_comb_agg)\n",
    "print(cv_score(active0_comb_agg, active0_mlp))\n",
    "\n",
    "active1_mlp = mlp_train(active1_comb_agg)\n",
    "print(cv_score(active1_comb_agg, active1_mlp))\n",
    "\n",
    "active2_mlp = mlp_train(active2_comb_agg)\n",
    "print(cv_score(active2_comb_agg, active2_mlp))\n",
    "\n",
    "postactive0_mlp = mlp_train(postactive0_comb_agg)\n",
    "print(cv_score(postactive0_comb_agg, postactive0_mlp))\n",
    "\n",
    "postactive1_mlp = mlp_train(postactive1_comb_agg)\n",
    "print(cv_score(postactive1_comb_agg, postactive1_mlp))\n",
    "\n",
    "postactive2_mlp = mlp_train(postactive2_comb_agg)\n",
    "print(cv_score(postactive2_comb_agg, postactive2_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-307.87850098944585\n",
      "-39.83524784406326\n",
      "-11.539645599794712\n",
      "-16.489164564092192\n",
      "-14.031248975511156\n",
      "-3.2605204895680844\n",
      "-15.487583274414497\n",
      "-1.2461372641353388\n",
      "0.2882031061193261\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preactive0_rf = rf_train(preactive0_comb_agg)\n",
    "print(cv_score(preactive0_comb_agg, preactive0_rf_model))\n",
    "\n",
    "preactive1_rf = rf_train(preactive1_comb_agg)\n",
    "print(cv_score(preactive1_comb_agg, preactive1_rf_model))\n",
    "\n",
    "preactive2_rf = rf_train(preactive2_comb_agg)\n",
    "print(cv_score(preactive2_comb_agg, preactive2_rf_model))\n",
    "\n",
    "active0_rf = rf_train(active0_comb_agg)\n",
    "print(cv_score(active0_comb_agg, active0_rf_model))\n",
    "\n",
    "active1_rf = rf_train(active1_comb_agg)\n",
    "print(cv_score(active1_comb_agg, active1_rf_model))\n",
    "\n",
    "active2_rf = rf_train(active2_comb_agg)\n",
    "print(cv_score(active2_comb_agg, active2_rf_model))\n",
    "\n",
    "postactive0_rf = rf_train(postactive0_comb_agg)\n",
    "print(cv_score(postactive0_comb_agg, postactive0_rf_model))\n",
    "\n",
    "postactive1_rf = rf_train(postactive1_comb_agg)\n",
    "print(cv_score(postactive1_comb_agg, postactive1_rf_model))\n",
    "\n",
    "postactive2_rf = rf_train(postactive2_comb_agg)\n",
    "print(cv_score(postactive2_comb_agg, postactive2_rf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(df, model):\n",
    "    df = df.drop(['future_tweets'], axis=1)\n",
    "    \n",
    "    next_hour = df.index[-1:]\n",
    "    next_hour_index = next_hour.shift(1, freq='H')\n",
    "    next_hour_index = next_hour_index.strftime(\"%Y-%m-%d %H:%M:%S-08:00\")\n",
    "    next_hour = str(df.index[-1])\n",
    "\n",
    "    s = df.xs(next_hour)\n",
    "    s.name = str(next_hour_index[0]) \n",
    "    \n",
    "    df = df.append(s)\n",
    "    df['hour'][-1] += 1\n",
    "    \n",
    "    if(df['tweets'][-2] <= df['tweets'][-3]):\n",
    "        df['tweets'][-1] = df['tweets'][-2] - (df['tweets'][-3] - df['tweets'][-2])\n",
    "    else:\n",
    "        df['tweets'][-1] = df['tweets'][-2] + (df['tweets'][-2] - df['tweets'][-3])\n",
    "    if(df['retweets'][-2] <= df['retweets'][-3]):\n",
    "        df['retweets'][-1] = df['retweets'][-2] - (df['retweets'][-3] - df['retweets'][-2])\n",
    "    else:\n",
    "        df['retweets'][-1] = df['retweets'][-2] + (df['retweets'][-2] - df['retweets'][-3])\n",
    "    if(df['followers'][-2] <= df['followers'][-3]):\n",
    "        df['followers'][-1] = df['followers'][-2] - (df['followers'][-3] - df['followers'][-2])\n",
    "    else:\n",
    "        df['followers'][-1] = df['followers'][-2] + (df['followers'][-2] - df['followers'][-3])\n",
    "    if(df['followers_max'][-2] <= df['followers_max'][-3]):\n",
    "        df['followers_max'][-1] = df['followers_max'][-2] - (df['followers_max'][-3] - df['followers_max'][-2])\n",
    "    else:\n",
    "        df['followers_max'][-1] = df['followers_max'][-2] + (df['followers_max'][-2] - df['followers_max'][-3])\n",
    "\n",
    "    return model.predict(df)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.8\n",
      "846.0\n",
      "98.1\n",
      "1145.4\n",
      "924.2\n",
      "212.1\n",
      "77.0\n",
      "35.7\n",
      "33.339999999999996\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(print_prediction(preactive0_comb_agg, preactive0_rf))\n",
    "print(print_prediction(preactive1_comb_agg, preactive1_rf))\n",
    "print(print_prediction(preactive2_comb_agg, preactive2_rf))\n",
    "\n",
    "print(print_prediction(active0_comb_agg, active0_rf))\n",
    "print(print_prediction(active1_comb_agg, active1_rf))\n",
    "print(print_prediction(active2_comb_agg, active2_rf))\n",
    "\n",
    "print(print_prediction(postactive0_comb_agg, postactive0_rf))\n",
    "print(print_prediction(postactive1_comb_agg, postactive1_rf))\n",
    "print(print_prediction(postactive2_comb_agg, postactive2_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
